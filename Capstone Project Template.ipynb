{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "I am doing this project based on data that I have put together. I will describe the data and its sources in Step 1.\n",
    "\n",
    "#### Project Summary\n",
    "InvestSure is an investment company that manages the retirement accounts of employees of its customers. It gets a dump of many data elements in CSV format from transactional systems and has been using Excel to load this data for analysis. However, the data has now grown to a size where this approach is no longer viable. Therefore, InvestSure has hired me as a Data Engineer to analyze this data, cleanse it, build a conceptual model for analytical use of the data and load the data from Excel files into the analytical tables. InvestSure has also requested me to provide them with typical queries that they could run on this analytical model to gain insights into this data.\n",
    "\n",
    "The project follows the steps listed below:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_data/\n"
     ]
    }
   ],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import configparser\n",
    "from datetime import datetime, timedelta, date\n",
    "from dateutil import parser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear\n",
    "\n",
    "# To suppress numeric values from being returned in exponential format\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "\n",
    "# suppress warnings from final output\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# Read project data configuration entries\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('capstone_project_data.cfg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project is to analyze the data provided by transactional systems, cleanse the data if needed and load it into an analytical data model to facilitate querying the data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "In this section, I will describe the data including its source.\n",
    "\n",
    "- txn.csv (fact)\n",
    "    - Structure\n",
    "        - txn_id\n",
    "        - txn_date\n",
    "        - contact_id\n",
    "        - product_id\n",
    "        - sales\n",
    "        - redemptions\n",
    "    - Source: InvestSure's transactional system captured by its trading application\n",
    "    - Feed frequency: Daily\n",
    "\n",
    "- customer.csv (dimension)\n",
    "    - Structure\n",
    "        - customer_id\n",
    "        - customer_name\n",
    "        - sector\n",
    "    - Source: InvestSure's CRM system\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- contact.csv (dimenstion)\n",
    "    - Structure\n",
    "        - contact_id\n",
    "        - first_name\n",
    "        - last_name\n",
    "        - city\n",
    "        - state_code\n",
    "        - zip\n",
    "        - country\n",
    "        - latitude\n",
    "        - longitude\n",
    "        - customer_id\n",
    "        - status\n",
    "        - opportunity\n",
    "    - Source: InvestSure's CRM system\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- product.json (dimension)\n",
    "    - Structure\n",
    "        - product_id\n",
    "        - product_name\n",
    "        - tna\n",
    "        - ms_rating\n",
    "        - exp_ratio\n",
    "        - market_cap\n",
    "    - Source: Yahoo Finance\n",
    "    - Feed Frequency: Daily\n",
    "\n",
    "- sec_codes.csv (mapping table)\n",
    "    - Structure\n",
    "        - code\n",
    "        - description\n",
    "    - Source: Yahoo Finance provides description but InvestSure's systems use abbreviated codes\n",
    "    - Feed Frequency: On demand and when new customers are added\n",
    "\n",
    "- state.csv\n",
    "    - Structure\n",
    "        - state_code\n",
    "        - state\n",
    "        - region\n",
    "    - Source: US Census Board\n",
    "    - Feed Frequency: one time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-2b1344f2": {
       "style": "primary"
      }
     }
    }
   },
   "outputs": [],
   "source": [
    "# Create a Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- sector: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CUSTOMER data\n",
    "df_customer = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_CUSTOMER'])\n",
    "\n",
    "df_customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------+\n",
      "|customer_id|       customer_name|sector|\n",
      "+-----------+--------------------+------+\n",
      "|  450056063|B.W.E CUSTOM CONS...|    HC|\n",
      "|  450056064|SERGEY NIZHEGOROD...|    CS|\n",
      "|  450056066| SUNRISE ANDOVER LLC|    RE|\n",
      "|  450056067|474 CENTRAL BOULE...|    FS|\n",
      "|  450056068|  ELITE FINISHES LLC|  TECH|\n",
      "+-----------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from CUSTOMER data\n",
    "df_customer.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- contact_id: integer (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- opportunity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CONTACT data\n",
    "df_contact = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_CONTACT'])\n",
    "\n",
    "df_contact.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "|contact_id|first_name| last_name|       city|state_code|  zip|country|latitude|longitude|customer_id|status|opportunity|\n",
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "| 100000339|     Lyndy|   Chachas|      Omaha|        NE|68130|    USA|41.22962| -96.1815|  450058148|Active|      50000|\n",
      "| 100001423|     Watts|Eifenstadt|     Weston|        FL|33326|    USA|26.09966|-80.36497|  450059017|Active|      50000|\n",
      "| 100001837|  Jingfeng|    Lopina|Hunt Valley|        MD|21030|    USA|39.50004|-76.66566|  450059076|Active|     125000|\n",
      "| 100002544|   Gaynell|   Vivrett|     Beloit|        WI|53511|    USA|42.49625|-89.03702|  450057762|Active|      16000|\n",
      "| 100002551| Peregrino|    Valles|   New York|        NY|10036|    USA|40.75841|-73.98155|  450055995|Active|     150000|\n",
      "+----------+----------+----------+-----------+----------+-----+-------+--------+---------+-----------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from CONTACT data\n",
    "df_contact.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- exp_ratio: double (nullable = true)\n",
      " |-- market_cap: string (nullable = true)\n",
      " |-- ms_rating: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- tna: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read PRODUCT data (note that this data is in JSON format)\n",
    "df_product = spark.read.format('json').options(header='true', inferSchema='true', multiline='true').load(config['LOCAL']['INPUT_DATA_PRODUCT'])\n",
    "\n",
    "df_product.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "|exp_ratio|market_cap|ms_rating|product_id|        product_name|         tna|\n",
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "|     0.05|     Giant|        5|     VFIAX|Vanguard 500 Inde...|163456368456|\n",
      "|     0.05|     Giant|        4|     VTSAX|Vanguard Total St...|136131758268|\n",
      "|     0.04|     Giant|        5|     VINIX|Vanguard Institut...|110407917518|\n",
      "|     0.16|     Giant|        4|     VTSMX|Vanguard Total St...| 98869371846|\n",
      "|     0.02|     Giant|        5|     VIIIX|Vanguard Institut...| 93192353649|\n",
      "+---------+----------+---------+----------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from PRODUCT data\n",
    "df_product.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read SECTOR CODES data\n",
    "df_sec_codes = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_SECTOR'])\n",
    "\n",
    "df_sec_codes.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|         description|\n",
      "+----+--------------------+\n",
      "|  FS|  Financial Services|\n",
      "|  RE|         Real Estate|\n",
      "|  HC|          Healthcare|\n",
      "|  UT|           Utilities|\n",
      "|  CS|Communication Ser...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from SECTOR CODE data\n",
    "df_sec_codes.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read STATE data\n",
    "df_state = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_STATE'])\n",
    "\n",
    "df_state.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|state_code|     state|  region|\n",
      "+----------+----------+--------+\n",
      "|        AL|   Alabama|Southern|\n",
      "|        AK|    Alaska| Pacific|\n",
      "|        AZ|   Arizona| Pacific|\n",
      "|        AR|  Arkansas|Southern|\n",
      "|        CA|California| Pacific|\n",
      "+----------+----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from STATE data\n",
    "df_state.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- txn_id: integer (nullable = true)\n",
      " |-- txn_date: string (nullable = true)\n",
      " |-- contact_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      " |-- redemptions: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read TRANSACTION data\n",
    "df_txn = spark.read.format('csv').options(header='true', inferSchema='true').load(config['LOCAL']['INPUT_DATA_TXN'])\n",
    "\n",
    "df_txn.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+----------+---------+-----------+\n",
      "|   txn_id|txn_date|contact_id|product_id|    sales|redemptions|\n",
      "+---------+--------+----------+----------+---------+-----------+\n",
      "|422909780|  1/2/15| 992808564|     VIVAX|46892.193|        0.0|\n",
      "|422909781|  1/2/15| 261785827|     SOPAX|      0.0| -33424.776|\n",
      "|422909782|  1/2/15| 389127962|     BAICX|14230.848|        0.0|\n",
      "|422909783|  1/2/15| 101692476|     SGROX|94046.931|        0.0|\n",
      "|422909784|  1/2/15| 327754553|     FXSIX|22038.856|        0.0|\n",
      "+---------+--------+----------+----------+---------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display sample rows from TRANSACTION data\n",
    "df_txn.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract columns to create CUSTOMER table\n",
    "customer_columns = [\"customer_id\", \"customer_name\", \"sector\"]\n",
    "\n",
    "# Write CUSTOMER table to parquet file\n",
    "customer_table = df_spark_customer.select(customer_columns)\n",
    "customer_table.write.parquet(config['LOCAL']['OUTPUT_DATA_CUSTOMER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "List the steps necessary to pipeline the data into the chosen data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
